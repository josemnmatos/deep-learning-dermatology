{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import CustomModelPipelineCorrected\n",
    "from custom_models.mlp import MLP  \n",
    "from helper_methods import get_data_loaders  \n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import torch\n",
    "import pandas as pd \n",
    "\n",
    "# set seaborn style\n",
    "sns.set_theme(\n",
    "    context=\"notebook\", style=\"whitegrid\", palette=\"bright\", color_codes=True, rc=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Section ---\n",
    "\n",
    "# Parameters for the search\n",
    "N_EPOCHS_SEARCH = (\n",
    "    5  # Max epochs for hyperparameter search runs (early stopping applies)\n",
    ")\n",
    "N_EPOCHS_FINAL = 200  # Max epochs for the final model training\n",
    "ES_PATIENCE = 5  # Early stopping patience\n",
    "\n",
    "# Define Hyperparameter Search Space\n",
    "# Experiment 1: Hidden Layer Configurations (Size and Depth combined for simplicity here)\n",
    "# You can structure this differently if you want strict separation like before\n",
    "layer_configs = {\n",
    "    \"HL_256_64\": (256, 64),\n",
    "    \"HL_512_128\": (512, 128),\n",
    "    \"HL_1024_256\": (1024, 256),\n",
    "    \"HL_2048_512\": (2048, 512),\n",
    "    \"HL_4096_1024\": (4096, 1024),\n",
    "    \"HL_1024\": (1024,),  # From Depth Exp\n",
    "    \"HL_1024_512_256\": (1024, 512, 256),  # From Depth Exp\n",
    "    # Add more configurations if needed\n",
    "}\n",
    "\n",
    "# Experiment 2: Optimizers and Learning Rates\n",
    "optimizers_config = {\n",
    "    \"Adam_0.005\": (optim.Adam, 0.005),\n",
    "    \"Adam_0.001\": (optim.Adam, 0.001),\n",
    "    \"RMSprop_0.005\": (optim.RMSprop, 0.005),\n",
    "    \"RMSprop_0.001\": (optim.RMSprop, 0.001),\n",
    "}\n",
    "\n",
    "# Experiment 3: Loss Functions (Optional - if you want to search this)\n",
    "# criterion_config = {\n",
    "#     \"CrossEntropy\": nn.CrossEntropyLoss,\n",
    "#     \"MultiMargin\": nn.MultiMarginLoss,\n",
    "# }\n",
    "# For now, let's stick to CrossEntropyLoss as indicated by your previous findings\n",
    "criterion_to_use = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedMNIST v3.0.2 @ https://github.com/MedMNIST/MedMNIST/\n",
      "Using downloaded and verified file: C:\\Users\\josem\\.medmnist\\dermamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\josem\\.medmnist\\dermamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\josem\\.medmnist\\dermamnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\josem\\.medmnist\\dermamnist.npz\n",
      "Input size: 2352, Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading ---\n",
    "(\n",
    "    train_loader,\n",
    "    train_loader_at_eval,\n",
    "    test_loader,\n",
    "    validation_loader,\n",
    "    n_channels,\n",
    "    n_classes,\n",
    "    task,\n",
    "    pil_dataset,\n",
    ") = get_data_loaders()\n",
    "\n",
    "# Calculate input size\n",
    "sample = next(iter(train_loader))\n",
    "input_size = n_channels * sample[0].shape[2] * sample[0].shape[3]\n",
    "num_classes = n_classes\n",
    "print(f\"Input size: {input_size}, Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_for_config(layer_config, opt_class, lr, criterion):\n",
    "    \"\"\"Initialize pipeline with MLP model for a specific configuration\"\"\"\n",
    "    # Initialize MLP with proper parameters\n",
    "    model = MLP(\n",
    "        input_size=input_size, hidden_sizes=layer_config, num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer for this specific model instance\n",
    "    optimizer = opt_class(model.parameters(), lr=lr)\n",
    "\n",
    "    pipeline = CustomModelPipelineCorrected(\n",
    "        model=model,\n",
    "        criterion=criterion(),  \n",
    "        optimizer=optimizer,  \n",
    "        n_epochs=N_EPOCHS_SEARCH, \n",
    "        training_data=train_loader,\n",
    "        validation_data=validation_loader,\n",
    "        test_data=test_loader,\n",
    "        patience=ES_PATIENCE,\n",
    "        min_delta=0.001,\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
